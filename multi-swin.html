<!DOCTYPE html>
<html lang="en">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1.0"/>
  <link rel="icon" type="image/x-icon" href="images/YH.png" />
  <title>Multi-Swin</title>
  <!--Import Google Icon Font-->

  <!--<link href="http://fonts.useso.com/icon?family=Material+Icons" rel="stylesheet">-->
  <link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">
  <!--Import materialize.css-->
  <link type="text/css" rel="stylesheet" href="css/materialize.min.css"  media="screen,projection"/>
  <link type="text/css" rel="stylesheet" href="css/main.css">
  <!--Let browser know website is optimized for mobile-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <!--Import jQuery before materialize.js-->
  <script type="text/javascript" src="js/jquery-3.0.0.min.js"></script>
  <script type="text/javascript" src="js/materialize.min.js"></script>
  <script type="text/javascript" src="js/main.js"></script>
  <style>
    body
    {
      font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
    }
    .vertical-nav
    {
      margin: 0;
      position: fixed;
      width: 300px;
      /*background-color: #343131;*/
      min-height: 100%;
      background-image: url("images/vertical_Nov19.jpg");
      background-repeat: round;
    }

    .profile-block
    {
      position: relative;
      height: 300px;
      /*background-color: #324D5C;*/

    }
    .profile-block-sm
    {
      position: relative;
      height: 200px;
      /*background-color: #324D5C;*/

    }

    .profile
    {
      position: absolute;
      top: 50%;
      transform: translateY(-50%);
      border-radius: 50%;
      margin-left: 30px;

      border: solid 5px #F4D03F;
    }

    .profile-sm
    {
      /*position: absolute;*/
      /*top: 50%;*/
      /*left: 50%;*/
      /*transform: translateX(50%);*/
      border-radius: 50%;
      margin: auto;

      border: solid 5px #F4D03F;
    }

    .logo-link
    {
      padding-left: 10px;
      padding-right: 10px;
    }
    .card-content
    {
      padding: 0 !important;
    }

    .card-title
    {
      background-color: #FBD8B0;
      padding: 12px;
    }

    p
    {
      line-height: 150%;
    }
    li
    {
      padding-bottom: 10px;
    }
    strong
    {
      font-weight: bolder;
    }
    .project-item
    {
    }
    .project-logo
    {
    }
    .authors{
      text-align: center;
      width: 100%;
      margin: auto;
      font-size: 12pt;
      padding-top: 20px;
      padding-bottom: 20px;
    }
    .author{
      font-weight: bold;
      display: inline;
      padding: 0 1em;
    }
    h3{
      font-size: xx-large;
      display: block;
      /*font-size: 1.5em;*/
      margin-block-start: 0.83em;
      margin-block-end: 0.83em;
      margin-inline-start: 0px;
      margin-inline-end: 0px;
      font-weight: bold;
    }
    .bibtex {
      padding-left: 20px;
      font-family: Courier New, Courier, 'gandhi_sansregular', monospace;
      white-space: pre;
    }
    ul.marked {list-style-type: circle !important;}
    ul li::marker {
      color: darkslategrey;
      font-size: 1.5em;
    }
  </style>
</head>
<body>

<div class="">
  <div style="">
    <div class="container">
      <div class="">
        <div class="row">
          <div class="col s12 m12 l10 offset-l1">
            <div class="title">
              <h2 class="center" style="margin-top: 120px; font-size: 32pt">
                Multi-Swin Transformer based Spatio-Temporal Information Exploration for Compressed Video Quality Enhancement
              </h2>
              <!-- <p class="center" style="font-size: large"><a href="https://arxiv.org/abs/2002.03711">[ arxiv.org/abs/2002.03711 ]</a> -->
                <p class="center" style="font-size: large"><i>2024, IEEE Signal Processing Letters（SPL）</i>
                <p class="center" style="font-size: large"><a href="https://ieeexplore.ieee.org/document/10599772/authors#authors">[ Paper ]</a>&nbsp<a href="https://github.com/Vencoders/Multi-Swin">[ Code ]</a>&nbsp<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10599772">[ PDF ]</a>
            </div>
           <div class="authors">
<!-- 			  <div class="author"><a href="https://huzi96.github.io/">Yueyu Hu</a></div><br class="hide-on-med-and-up"> -->
              <div class="author">Li Yu {li.yu@nuist.edu.cn}</a></div><br class="hide-on-med-and-up">
              <div class="author">Shiyu Wu {202212210022@nuist.edu.cn}</a></div><br class="hide-on-med-and-up">
	      <div class="author">Moncef Gabbouj {moncef.gabbouj@tuni.fi}</a></div><br class="hide-on-med-and-up">
            </div>
			<div class="abstract">
			  <h3 class="center">Abstract</h3>
			  <ul style="font-size: 13pt; text-align: justify">
			    <li>
			      <div>
			        <i class="material-icons tiny cyan-text">grade</i>
              Spatio-temporal information plays an important role in compressed video quality enhancement. Most advanced studies use deformable convolution or Swin transformer to explore spatio-temporal information. However, deformable convolution based methods may incur inaccurate motion compensation due to the compression artifacts and limited receptive fields. The Swin transformer based approaches are unable to fully explore the spatio-temporal information, limited by its rigid window-based mechanism. To solve the above problems, we propose a novel multi-Swin transformer-based network for compressed video quality enhancement to better explore spatio-temporal information. The whole workflow consists of the Local Alignment (LA) Module, the Global Refinement Fusion (GRF) Module, and the Quality Enhancement (QE) Module. The LA module roughly perceives the local motion through the deformable fusion. Subsequently, the GRF module employs the proposed multi-Swin transformer to enhance the spatio-temporal perception. Finally, the QE module effectively restores the texture details across various scales. Extensive experimental results prove the effectiveness of the proposed method.
			      </div>
			        </li>
			  </ul>
			</div>
	    <div class="abstract">
              <h3 class="center">Highlights</h3>
              <ul style="font-size: 13pt; text-align: justify">
                <li>
                  <div>
                    <i class="material-icons tiny cyan-text">grade</i>
                    We design a novel multi-Swin transformer that effectively identifies corresponding instance information from neighboring frames for spatio-temporal information exploration. 
                  </div>
                    </li>
                <li>
                  <div>
                    <i class="material-icons tiny  cyan-text">grade</i>
                  We propose the MSFS block which can exploit multi-scale spatial information.
                  </div>
                    </li>
                <li>
                  <div>
                    <i class="material-icons tiny  cyan-text">grade</i>
                    Extensive experiments verify that our network achieves superior performance.
                     </li>
              </ul>
            </div>
            <!-- <div class="abstract">
              <h3 class="center">Highlights</h3>
              <ul style="font-size: 13pt; text-align: justify">
                <li>
                  <div>
                    <i class="material-icons tiny cyan-text">grade</i>
                    We introduce reference-based SR in down/up-sampling based video coding method, where target and reference images are not required to be texture-aligned as required in existing methods.
                  </div>
                    </li>
                <li>
                  <div>
                    <i class="material-icons tiny  cyan-text">grade</i>
                    We proposed an adaptive group of pictures (GOP) method to automatically decide the adaptive sampling scheme.
                  </div>
                    </li>
                <li>
                  <div>
                    <i class="material-icons tiny  cyan-text">grade</i>
                   The neural texture transfer model for reference-based SR produces realistic up-sampled frame at the decoding end.
                  </div>
                     </li>
              </ul>
            </div> -->
            <div class="experiment">

              <div class="row">
                <h3 class="center">Network Architecture</h3>
                <div class="col l12 m12 s12">
                  <img src="images/multi-swin_net.png" class="responsive-img">
                </div>

                <div class="col l12 m12 s12">
                  <p style="text-align: justify"><i class="material-icons tiny cyan-text">grade</i> The target frame along with its six adjacent frames are sequentially input to enable the LA module and the GRF module to learn the spatio-temporal feature residuals. Subsequently, the QE module learns the residuals specific to the target frame, which are then added back to enhance the quality of the target frame. The GRF module comprises N AMST blocks. The AMST block undergoes convolution, and includes four transformers with varying window shifts to calculate the adaptive residuals. The QE module contains M MSFS blocks. The MSFS block handles three types of features at different expansion rates, employing pooling and convolution to selectively enhance useful channels within each feature. 
                  </p>
                </div>
              </div>
			  
             <div class="row">
                <h3 class="center">Results</h3>
                <div class="col l12 m12 s12">
                  <img src="images/multi-swin_result1.png" class="responsive-img">
				          <img src="images/yu3.png" class="responsive-img">
                </div>
			
			
				  
              <div class="row">
                <div class="col l12 m12 s12">
                  <p><strong>Please check our paper for detail results.</strong></p>
                </div>
              </div>
            </div>

              <div class="row">
				  <h3 class="center">Citation</h3>
                <div class="col l12 m12 s12">
                  <p>
                    @ARTICLE{10599772,
			author={Yu, Li and Wu, Shiyu and Gabbouj, Moncef},
			journal={IEEE Signal Processing Letters}, 
			title={Multi-Swin Transformer Based Spatio-Temporal Information Exploration for Compressed Video Quality Enhancement}, 
			year={2024},
			volume={31},
			number={},
			pages={1880-1884},
			keywords={Transformers;Convolution;Video recording;Quality assessment;Motion compensation;Feature extraction;Correlation;Compressed video quality enhancement;spatio-temporal information;swin transformer},
			doi={10.1109/LSP.2024.3429008}}
                 </p>
                </div>
              </div>
            </div>

            
            
          </div>
        </div>
      </div>
    </div>
  </div>
</div>
</body>
</html>
