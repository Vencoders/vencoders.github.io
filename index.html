<!DOCTYPE html>
<html lang="en">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1.0"/>
  <link rel="icon" type="image/x-icon" href="images/YH.png" />
  <title>Project</title>
  <!--Import Google Icon Font-->

  <!--<link href="http://fonts.useso.com/icon?family=Material+Icons" rel="stylesheet">-->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  <!--Import materialize.css-->
  <link type="text/css" rel="stylesheet" href="css/materialize.min.css"  media="screen,projection"/>
  <link type="text/css" rel="stylesheet" href="css/main.css">

  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <!--Import jQuery before materialize.js-->

  <script type="text/javascript" src="js/jquery-3.0.0.min.js"></script>
  <script type="text/javascript" src="js/materialize.min.js"></script>
  <script type="text/javascript" src="js/main.js"></script>
  <style>
    body
    {
      font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
    }
    .vertical-nav
    {
      margin: 0;
      position: fixed;
      width: 300px;
      /*background-color: #343131;*/
      min-height: 100%;
      background-image: url("images/vertical_oct24.jpg");
      background-repeat: round;
    }

    .profile-block
    {
      position: relative;
      height: 300px;
      /*background-color: #324D5C;*/
    }
    .profile-block-sm
    {
      position: relative;
      height: 200px;
      /*background-color: #324D5C;*/

    }
    .profile-block-tiny
    {
      position: relative;
      height: 50px;
      /*background-color: #324D5C;*/
    }

    .profile
    {
      position: absolute;
      top: 50%;
      transform: translateY(-50%);
      border-radius: 50%;
      margin-left: 30px;

      border: solid 5px #F4D03F;
    }

    .profile-sm
    {
      /*position: absolute;*/
      /*top: 50%;*/
      /*left: 50%;*/
      /*transform: translateX(50%);*/
      border-radius: 50%;
      margin: auto;

      border: solid 5px #F4D03F;
    }

    .profile-tiny
    {
      position: relative;
      /* top: 50%; */
      transform: translateY(-50%);
      border-radius: 50%;
      margin-left: 20px;

      border: solid 5px #F4D03F;
    }

    .logo-link
    {
      padding-left: 10px;
      padding-right: 10px;
    }
    .card-content
    {
      padding: 0 !important;
      font-size: large
    }

    .card-title
    {
      background-color: #FBD8B0;
      padding: 12px;
    }

    p
    {
      line-height: 150%;
      text-align: justify;
    }
    li
    {
      padding-bottom: 10px;
    }
    strong
    {
      font-weight: bolder;
    }
    .project-item
    {
    }
    .project-logo
    {
    }
  </style>
</head>
<body>
<div class="vertical-nav hide-on-med-and-down">
  <div class="profile-block">
    <img src="images/logo3.png" width="240px" class="profile">
  </div>
  <div style="color: #F8F8f8; font-size: large; margin-top: -30px" class="center-align">
    <p style="" class="center">Vencoders</p>
    <p style="font-size: medium;" class="center">Nanjing University of Information Science and Technology</p>
    <!-- <p style="padding-bottom: 30px; font-size: medium;" class="center"><i class="fa fa-envelope"></i> yyhu AT nyu&middot;edu</p> -->
<!--    <span class="logo-link">
            <a href="https://scholar.google.com/citations?user=MkWer14AAAAJ">
                <img src="images/google-scholar-logo.png" style="width: 30px">
            </a>
        </span>
    <span class="logo-link">
            <a href="https://instagram.com/minoshirod/">
                <img src="images/Instagram.png" style="width: 30px">
            </a>
        </span> -->
    <span class="logo-link">
            <a href="https://github.com/Vencoders">
                <img src="images/GitHub-Mark-Light-120px-plus.png" style="width: 30px">
            </a>
        </span>

<!--    <span class="logo-link">
            <a href="https://www.facebook.com/yueyuhu">
                <img src="images/FB-f-Logo__white_100.png" style="width: 30px">
            </a>
        </span> -->
  </div>

</div>


<div class="hide-on-med-and-down">
  <div style="padding-left: 300px">
    <div class="row" style="">
      <div class="container" style="width: 80%;">
        <div class="row">
          <div class="col s12 m12 l12">
            <div class="card">
              <div class="card-content" style="color: #30505D;">
                <span class="card-title" style="">About Us</span>
                <p style="padding: 24px; font-size: large;">1.School of Computer Science, Nanjing University of Information Science and Technology, Nanjing, China, <br>
									2.Engineering Research Center of Digital Forensics, Ministry of Education, Nanjing University of Information Science and Technology, Nanjing, China.
                  
              </div>
            </div>
          </div>

         
          <div class="col s12 m12 l12">
            <div class="card">
              <div class="card-content" style="color: #30505D;">
                <span class="card-title" style="">Projects</span>
                <div class="project-item">

		<h3>2025</h3>
                    <table>

		 <tr>
                        <td style="width: 30%">
                          <div class="project-logo">
                            <a href="https://vencoders.github.io/TAVDiff.html"><img src="images/TAVDiff-model.png"
                                style="width: 100%"></a>
                          </div>
                        </td>
                        <td style="width: 70%;">
                          <div class="">
                            <p style="font-size: 18pt; text-align: left;">
                              Text-Audio-Visual-conditioned Diffusion Model for Video Saliency Prediction
                            </p>
                            <p style="font-size: 10pt; text-align: left;">
                              Under Review
                            </p>
                            <p>
				Video saliency prediction is crucial for downstream applications, such as video compression and human-computer interaction. With the flourishing of multimodal learning, researchers increasingly explored multimodal video saliency prediction, including audio-visual and text-visual methods. Auditory cues attract the viewer's attention to sound sources, while textual cues offer semantic insights of video content which is aligned with human cognitive process. Integrating these complementary cues can improve the accuracy of saliency prediction. Therefore, we attempt to simultaneously analyze visual, auditory, and textual modalities in this paper, and propose TAVDiff, a Text-Audio-Visual-conditioned Diffusion Model for video saliency prediction. TAVDiff treats video saliency prediction as an image generation task conditioned on text, audio, and video inputs, and predicts saliency maps through stepwise denoising. To effectively utilize textual information, we employ a large multimodal model to generate textual descriptions for video frames and introduce the Saliency-Oriented Image-Text Response (SITR) mechanism. This generates image-text response maps that guide the model to focus on visual regions semantically related to the textual description. For auditory input, we use it as one of the conditional information to direct the model's attention toward salient regions indicated by sounds.  Additionally, since the diffusion transformer (DiT) typically concatenates conditional information with the timestep, potentially affecting noise estimation. We propose Saliency-DiT, which decouples conditional information from the timestep, enhancing effective conditional guidance. Experimental results show that TAVDiff outperforms existing methods, with an improvement of 1. 03%, 2. 35%, 2. 71% and 0. 33% in SIM, CC, NSS and AUC-J metrics, respectively.
                            </p>
                            <p>
                              <a href="https://vencoders.github.io/TAVDiff.html">[Project]</a>
                              <a href="https://arxiv.org/abs/2504.14267">[PDF]</a>
                              <a href="https://github.com/Vencoders/FANeRV">[Code]</a>
                            </p>
                          </div>
                        </td>
                      </tr>
			    
		 <tr>
                        <td style="width: 30%">
                          <div class="project-logo">
                            <a href="https://vencoders.github.io/AVRSP.html"><img src="images/AVRSP-model.png"
                                style="width: 100%"></a>
                          </div>
                        </td>
                        <td style="width: 70%;">
                          <div class="">
                            <p style="font-size: 18pt; text-align: left;">
                              Relevance-guided Audio Visual Fusion for Video Saliency Prediction
                            </p>
                            <p style="font-size: 10pt; text-align: left;">
                              Under Review
                            </p>
                            <p>
				Audio data, often synchronized with video frames, plays a crucial role in guiding the audience’s visual attention. Incorporating audio information into video saliency prediction task can enhance the prediction of human visual behavior. However, existing audio-visual saliency prediction methods often directly fuse audio and visual features, without evaluating the semantic relevance between them. Fusing visual features with semantically irrelevant audio (e.g., background music) will introduce noise and degrade prediction performance. To address this issue, we propose a novel Audio-Visual Relevance-guided Saliency Prediction network (AVRSP). Specifically, the Relevance-guided Audio-Visual Feature Fusion module (RAVF) dynamically adjusts the retention of audio features based on the semantic relevance between audio and visual elements, thereby refining the fusion process with visual features. Furthermore, the Multi-scale feature Synergy (MS) module integrates visual features from different encoding stages, enhancing the network’s ability to represent objects at various scales. The Multi-scale Regulator Gate (MRG) transfers crucial fusion information to visual features, thus optimizing the utilization of multi-scale visual features. Extensive experiments on six audio-visual eye movement datasets demonstrate that the proposed AVRSP network achieves superior performance to recent methods, with improvements of 1.93%, 3.05%, and 3.81% in SIM, CC, and NSS respectively.
                            </p>
                            <p>
                              <a href="https://vencoders.github.io/AVRSP.html">[Project]</a>
                              <a href="https://arxiv.org/abs/2411.11454">[PDF]</a>
                              <a href="https://github.com/Vencoders/FANeRV">[Code]</a>
                            </p>
                          </div>
                        </td>
                      </tr>
			    
		 <tr>
                        <td style="width: 30%">
                          <div class="project-logo">
                            <a href="https://vencoders.github.io/FANeRV.html"><img src="images/FA-model.jpg"
                                style="width: 100%"></a>
                          </div>
                        </td>
                        <td style="width: 70%;">
                          <div class="">
                            <p style="font-size: 18pt; text-align: left;">
                              FANeRV: Frequency Separation and Augmentation based Neural Representation for Video
                            </p>
                            <p style="font-size: 10pt; text-align: left;">
                              Under Review
                            </p>
                            <p>
				Neural representations for video (NeRV) have gained considerable attention for their strong performance across various video tasks. However, existing NeRV methods often struggle to capture fine spatial details, resulting in vague reconstructions. In this paper, we present a Frequency Separation and Augmentation based Neural Representation for video (FANeRV), which addresses these limitations with its core Wavelet Frequency Upgrade Block. This block explicitly separates input frames into high and low-frequency components using discrete wavelet transform, followed by targeted enhancement using specialized modules. Finally, a specially designed gated network effectively fuses these frequency components for optimal reconstruction. Additionally, convolutional residual enhancement blocks are integrated into the later stages of the network to balance parameter distribution and improve the restoration of high-frequency details. Experimental results demonstrate that FANeRV significantly improves reconstruction performance and excels in multiple tasks, including video compression, inpainting, and interpolation, outperforming existing NeRV methods.
                            </p>
                            <p>
                              <a href="https://vencoders.github.io/FANeRV.html">[Project]</a>
                              <a href="https://pan.baidu.com/s/10yGHhIJDlFuRzmIQE0k9CA?pwd=2yve">[PDF]</a>
                              <a href="https://github.com/Vencoders/FANeRV">[Code]</a>
                            </p>
                          </div>
                        </td>
                      </tr>


		 <tr>
                        <td style="width: 30%">
                          <div class="project-logo">
                            <a href="https://vencoders.github.io/HFE-NeRV.html"><img src="images/HFE-model.png"
                                style="width: 100%"></a>
                          </div>
                        </td>
                        <td style="width: 70%;">
                          <div class="">
                            <p style="font-size: 18pt; text-align: left;">
                              High-Frequency Enhanced Hybrid Neural Representation for Video Compression
                            </p>
                            <p style="font-size: 10pt; text-align: left;">
                              2025, Expert Systems with Applications
                            </p>
                            <p>
                              Neural Representations for Videos (NeRV) have simplified the video codec process and achieved swift decoding speeds by encoding video content into a neural network, presenting a promising solution for video compression. However, existing work overlooks the crucial issue that videos reconstructed by these methods lack high-frequency details. To address this problem, this paper introduces a High-Frequency Enhanced Hybrid Neural Representation Network. Our method focuses on leveraging high-frequency information to improve the synthesis of fine details by the network. Specifically, we design a wavelet high-frequency encoder that incorporates Wavelet Frequency Decomposer (WFD) blocks to generate high-frequency feature embeddings. Next, we design the High-Frequency Feature Modulation (HFM) block, which leverages the extracted high-frequency embeddings to enhance the fitting process of the decoder. Finally, with the refined Harmonic decoder block and a Dynamic Weighted Frequency Loss, we further reduce the potential loss of high-frequency information. Experiments on the Bunny and UVG datasets demonstrate that our method outperforms other methods, showing notable improvements in detail preservation and compression performance.

                            </p>
                            <p>
                              <a href="https://vencoders.github.io/HFE-NeRV.html">[Project]</a>
                              <a href="https://pan.baidu.com/s/1CHHRuKa0SpZN6D6JcNdl-g?pwd=96w4">[PDF]</a>
                              <a href="https://github.com/Vencoders/HFE-NeRV">[Code]</a>
                            </p>
                          </div>
                        </td>
                      </tr>


		 <tr>
                        <td style="width: 30%">
                          <div class="project-logo">
                            <a href="https://vencoders.github.io/PU-DT.html"><img src="images/PU-DTframework.png"
                                style="width: 100%"></a>
                          </div>
                        </td>
                        <td style="width: 70%;">
                          <div class="">
                            <p style="font-size: 18pt; text-align: left;">
                              Point Cloud Upsampling via Implicit Shape Priors Discovery and Refinement
                            </p>
                            <p style="font-size: 10pt; text-align: left;">
                              2025, Displays
                            </p>
                            <p>
                              The point clouds obtained by scanning sensors are often sparse and non-uniform, therefore,
                              point cloud upsampling is of vital importance. This paper considers geometric priors as a
                              rich source to guide point cloud generation for the better qualities. However, it is less
                              flexible to explicitly exploit geometric priors of object surface, such as local geometric
                              smoothness and fairness. In light of this, this paper proposes a novel two-stage method
                              via discovering and exploiting implicit shape priors, which can consist of coarse point
                              cloud upsampling and fine details refining. Specifically, at the first stage, we explore
                              to discover geometric priors in an implicit manner via Dual Transformer, which
                              simultaneously addressing local and global information during feature encoding, while a
                              Neighborhood Refinement module is proposed to handle with geometric irregularities and
                              noises via exploiting feature similarity of neighboring points. Extensive experiments on
                              synthetic and real datasets can verify our motivation, as our method can gain superior
                              performance to existing upsampling methods, especially with noisy point clouds.

                            </p>
                            <p>
                              <a href="https://vencoders.github.io/PU-DT.html">[Project]</a>
                              <a href="">[PDF]</a>
                              <a href="https://github.com/Vencoders/PU-DT">[Code]</a>
                            </p>
                          </div>
                        </td>
                      </tr>

		    <tr>
                        <td style="width: 30%">
                          <div class="project-logo">
                            <a href="https://vencoders.github.io/PCUDA-SGA.html"><img src="images/fram.png"
                                style="width: 100%"></a>
                          </div>
                        </td>
                        <td style="width: 70%;">
                          <div class="">
                            <p style="font-size: 18pt; text-align: left;">
                             Bridging Domain Gap of Point Cloud Representations via Self-Supervised Geometric Augmentation
                            </p>
                            <p style="font-size: 10pt; text-align: left;">
                              2025, IEEE Transactions on Circuits and Systems for Video Technology(TCSVT)
                            </p>
                            <p>
Recent progress of semantic point clouds analysis is largely driven by synthetic data (e.g., the ModelNet and the ShapeNet), which are typically complete, well-aligned and noisy-free. Therefore,  representations of those ideal synthetic point clouds have limited variations in the geometric perspective and can gain good performance on a number of 3D vision tasks such as point cloud classification. In the context of unsupervised domain adaptation (UDA), representation learning designed for synthetic point clouds can hardly capture domain invariant geometric patterns from incomplete and noisy point clouds. To address such a problem, we introduce a novel scheme for induced geometric invariance of point cloud representations across domains, via regularizing representation learning with two self-supervised geometric augmentation tasks. On one hand, a novel pretext task of predicting translation distances of augmented samples is proposed to alleviate centroid shift of point clouds due to occlusion and noises. On the other hand, we pioneer an integration of the self-supervised relational learning on geometrically-augmented point clouds in a cascade manner, utilizing the intrinsic relationship of augmented variants and other samples as extra constraints of cross-domain geometric features. Experiments on the PointDA-10 dataset demonstrate the effectiveness of the proposed method, achieving the state-of-the-art performance.
                            </p>
                            <p>
                              <a href="https://vencoders.github.io/PCUDA-SGA.html">[Project]</a>
                              <a href="https://ieeexplore.ieee.org/abstract/document/10820548">[PDF]</a>
                              <a href="https://github.com/Vencoders/PCUDA-SGA">[Code]</a>
                            </p>
                          </div>
                        </td>
                      </tr>
		
                      <tr>
                        <td style="width: 30%">
                          <div class="project-logo">
                            <a href="https://vencoders.github.io/PCUDA-MCC.html"><img src="images/RL.png"
                                style="width: 100%"></a>
                          </div>
                        </td>
                        <td style="width: 70%;">
                          <div class="">
                            <p style="font-size: 18pt; text-align: left;">
                             Unsupervised Domain Adaptation on Point Cloud Classification via Imposing Structural Manifolds into Representation Space
                            </p>
                            <p style="font-size: 10pt; text-align: left;">
                              2025, International Conference on Computational Visual Media(CVM)
                            </p>
                            <p>
			        In the context of unsupervised domain adaptation (UDA) for point cloud classification, deep classifiers training on data from source domain (e.g. clean synthetic point clouds) cannot perform well on those from target domain (e.g. noisy real-world ones),
which can be caused by a significant domain discrepancy of point representations. For closing domain gap, recent algorithms adopt the popular self-training strategies (e.g. self-paced self-training) but suffer from lack of imposing structural constraints into representation learning.
To tackle this issue, we propose a novel dual-augmentation relational learning scheme (i.e. introducing consistency regularization on augmented samples in both observation and feature space) to incorporate low-dimensional manifolds to encourage domain-invariant representations.
Moreover, we design a novel filtering mechanism that adaptively adjusts thresholds for each semantic category based on confidence distributions and validates neighborhood consistency to further mitigate feature ambiguities. 
Comprehensive experiments on the widely-used PointDA-10 dataset demonstrate that our method achieves the state-of-the-art performance. 
                            </p>
                            <p>
                              <a href="https://vencoders.github.io/PCUDA-MCC.html">[Project]</a>
                              <a href="https://pan.baidu.com/s/1KB_YcqB1HXr_dtJNJWXoCQ?pwd=cggq">[PDF]</a>
                              <a href="https://github.com/Vencoders/PCUDA-MCC">[Code]</a>
                            </p>
                          </div>
                        </td>
                      </tr>
                    </table>
			
                  <h3>2024</h3>
                    <table>
                      <tr>
                        <td style="width: 30%">
                          <div class="project-logo">
                            <a href="https://vencoders.github.io/selfvqa.html"><img src="images/svqa.png"
                                style="width: 100%"></a>
                          </div>
                        </td>
                        <td style="width: 70%;">
                          <div class="">
                            <p style="font-size: 18pt; text-align: left;">
                              基于自监督学习与多尺度时空特征融合的视频质量评估
                            </p>
                            <p style="font-size: 10pt; text-align: left;">
                              2024, 计算机系统应用
                            </p>
                            <p>
                              面对视频质量评估领域标记数据不足的问题, 研究者开始转向自监督学习方法, 旨在借助大量未标记数据以学习视频质量评估模型.
                              然而现有自监督学习方法主要聚焦于视频的失真类型和视频内容信息, 忽略了视频随时间变化的动态信息和时空特征, 这导致在复杂动态场景下的评估效果不尽人意. 针对上述问题,
                              提出了一种新的自监督学习方法, 通过播放速度预测作为预训练的辅助任务, 使模型能更好地捕捉视频的动态变化和时空特征, 并结合失真类型预测和对比学习,
                              增强模型对视频质量差异的敏感性学习. 同时, 为了更全面捕捉视频的时空特征, 进一步设计了多尺度时空特征提取模块等以加强模型的时空建模能力. 实验结果显示,
                              所提方法在LIVE、CSIQ以及LIVE-VQC数据集上, 性能显著优于现有的基于自监督学习的方法, 在LIVE-VQC数据集上, 本方法在PLCC指标上平均提升7.90%,
                              最高提升17.70%. 同样, 在KoNVid-1k数据集上也展现了相当的竞争力. 这些结果表明, 提出的自监督学习框架有效增强了视频质量评估模型的动态特征捕捉能力,
                              并在处理复杂动态视频中显示出独特优势.
                            </p>
                            <p>

                              <a href="https://vencoders.github.io/selfvqa.html">[Project]</a>
                              <a href="https://pan.baidu.com/s/1-cTy1Isl_p5yVr8ebDxabg?pwd=z6vl">[PDF]</a>
                              <a href="https://github.com/Vencoders/Self-supervised-video-quality-assessment">[Code]</a>
                            </p>
                          </div>
                        </td>
                      </tr>


                      <tr>
                        <td style="width: 30%">
                          <div class="project-logo">
                            <a href="https://vencoders.github.io/MAESR360.html"><img src="images/MAESR360_framework.png"
                                style="width: 100%"></a>
                          </div>
                        </td>
                        <td style="width: 70%;">
                          <div class="">
                            <p style="font-size: 18pt; text-align: left;">
                              MAESR360: Masked autoencoder-based 360-degree video streaming via multi-scale feature
                              fusion
                            </p>
                            <p style="font-size: 10pt; text-align: left;">
                              2024, Visual Communication and Image Processing Conference (VCIP)
                            </p>
                            <p>
                              360-degree video streaming is becoming increasingly popular for its immersive experience.
                              Traditional adaptive tile-based streaming methods allocate the bitrates according to
                              viewport prediction, which effectively reduces required transmission bandwidth, but it
                              will cause serious quality degradation when the viewport prediction is inaccurate. Thus,
                              some researchers propose visual reconstruction and enhancement-based 360-degree video
                              streaming framework, which can reconstructs the whole frame at very low bitrates. However,
                              existing frameworks are built upon image-based visual reconstruction methods, which do not
                              fully consider the characteristics of videos. In this paper, we propose a masked
                              autoencoder-based, multi-scale optimized framework for 360-degree video streaming
                              (MAESR360), which fully considers the temporal relevance of the video. We utilize
                              spatio-temporal downsampling and high-ratio tube masking strategies to effectively reduce
                              the amount of transmitted data. Additionally, we design a lightweight visual
                              reconstruction model based on multi-scale feature fusion to recover the visual quality of
                              video frames. The effectiveness of our proposed method is demonstrated through extensive
                              experiments.
                            </p>
                            <p>
                              <a href="https://vencoders.github.io/MAESR360.html">[Project]</a>
                              <a href="https://www.alipan.com/s/haKj2hy9tY2">[PDF]</a>
                              <a href="https://www.alipan.com/s/k37JQWxqYUk">[PPT]</a>
                              <a href="https://github.com/Vencoders/MAESR360">[Code]</a>
                            </p>
                          </div>
                        </td>
                      </tr>
                      

                      <tr>
                        <td style="width: 30%">
                          <div class="project-logo">
                            <a href="https://vencoders.github.io/JICD.html"><img src="images/overviewjicd.png"
                                style="width: 100%"></a>
                          </div>
                        </td>
                        <td style="width: 70%;">
                          <div class="">
                            <p style="font-size: 18pt; text-align: left;">
                              JOINT END-TO-END IMAGE COMPRESSION AND DENOISING: LEVERAGING CONTRASTIVE LEARNING AND
                              MULTI-SCALE SELF-ONNS
                            </p>
                            <p style="font-size: 10pt; text-align: left;">
                              2024, IEEE International Conference on Image Processing（ICIP）
                            </p>
                            <p>
                              Noisy images are a challenge to image compression algorithms due to the inherent
                              difficulty of compressing noise. As noise cannot easily be discerned from image details,
                              such as high-frequency signals, its presence leads to extra bits needed for compression.
                              Since the emerging learned image compression paradigm enables end-to-end optimization of
                              codecs, recent efforts were made to integrate denoising into the compression model,
                              relying on clean image features to guide denoising. However, these methods exhibit
                              suboptimal performance under high noise levels, lacking the capability to generalize
                              across diverse noise types. In this paper, we propose a novel method integrating a
                              multi-scale denoiser comprising of Self Organizing Operational Neural Networks, for joint
                              image compression and denoising. We employ contrastive learning to boost the network
                              ability to differentiate noise from high frequency signal components, by emphasizing the
                              correlation between noisy and clean counterparts. Experimental results demonstrate the
                              effectiveness of the proposed method both in ratedistortion performance, and codec speed,
                              outperforming the current state-of-the-art.
                            </p>
                            <p>
                              <a href="https://vencoders.github.io/JICD.html">[Project]</a>
                              <a href="https://arxiv.org/pdf/2402.05582">[PDF]</a>
                              <a href="https://github.com/Vencoders/Joint-image-denoising-and-compression">[Code]</a>
                            </p>
                          </div>
                        </td>
                      </tr>


                      <tr>
                        <td style="width: 30%">
                          <div class="project-logo">
                            <a href="https://vencoders.github.io/PU-AT.html"><img src="images/multi-swin_net.png"
                                style="width: 100%"></a>
                          </div>
                        </td>
                        <td style="width: 70%;">
                          <div class="">
                            <p style="font-size: 18pt; text-align: left;">
                              Multi-Swin Transformer based Spatio-Temporal Information Exploration for Compressed
                              Video
                              Quality Enhancement
                            </p>
                            <p style="font-size: 10pt; text-align: left;">
                              2024, IEEE Signal Processing Letters (SPL)
                            </p>
                            <p>
                              Spatio-temporal information plays an important role in compressed video quality
                              enhancement. Most advanced studies use deformable convolution or Swin transformer to
                              explore spatio-temporal information. However, deformable convolution based methods may
                              incur inaccurate motion compensation due to the compression artifacts and limited
                              receptive fields. The Swin transformer based approaches are unable to fully explore the
                              spatio-temporal information, limited by its rigid window-based mechanism. To solve the
                              above problems, we propose a novel multi-Swin transformer-based network for compressed
                              video quality enhancement to better explore spatio-temporal information. The whole
                              workflow consists of the Local Alignment (LA) Module, the Global Refinement Fusion (GRF)
                              Module, and the Quality Enhancement (QE) Module. The LA module roughly perceives the local
                              motion through the deformable fusion. Subsequently, the GRF module employs the proposed
                              multi-Swin transformer to enhance the spatio-temporal perception. Finally, the QE module
                              effectively restores the texture details across various scales. Extensive experimental
                              results prove the effectiveness of the proposed method.

                            </p>
                            <p>
                              <a href="https://vencoders.github.io/multi-swin.html">[Project]</a>
                              <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10599772">[PDF]</a>
                              <a href="https://github.com/Vencoders/Multi-Swin">[Code]</a>
                            </p>
                          </div>
                        </td>
                      </tr>


                     


                      <tr>
                        <td style="width: 30%">
                          <div class="project-logo">
                            <a href="https://vencoders.github.io/PU-AT.html"><img src="images/PU-AT1.png"
                                style="width: 100%"></a>
                          </div>
                        </td>
                        <td style="width: 70%;">
                          <div class="">
                            <p style="font-size: 18pt; text-align: left;">
                              Point cloud sampling method based on Transformer
                            </p>
                            <p style="font-size: 10pt; text-align: left;">
                              2024, 计算机与数字工程
                            </p>
                            <p>
                              we propose a novel 3D point cloud up-sampling method based on Transformers aiming to
                              explore the po-tential of Transformers in this domain. Initially, a two-step network is
                              employed, transitioning from rough dense point cloud generation to point cloud refinement,
                              with each sub-network focusing on specific objec-tives. The coarse generation network aims
                              to increase point cloud resolution, utilizing multiple densely connected convolutional
                              blocks to extract deep high-dimensional features. To efficiently explore the up-sampling
                              space of point clouds, an innovative integration of upsampling module with Transformer is
                              in-troduced. While the rough generation of point clouds increases the number of points,
                              correction of noise points is essential. In the refinement network, an adaptive refinement
                              module is introduced, capable of au-tonomously aggregating structural information based on
                              local point cloud characteristics.

                            </p>
                            <p>
                              <a href="https://vencoders.github.io/PU-AT.html">[Project]</a>
                              <a href="">[PDF]</a>
                              <a href="https://github.com/Vencoders/PU-AT">[Code]</a>
                            </p>
                          </div>
                        </td>
                      </tr>


                      <tr>
                        <td style="width: 30%">
                          <div class="project-logo">
                            <a href="https://vencoders.github.io/PII.html"><img src="images/f1.png"
                                style="width: 100%"></a>
                          </div>
                        </td>
                        <td style="width: 70%;">
                          <div class="">
                            <p style="font-size: 18pt; text-align: left;">
                              PANORAMIC IMAGE INPAINTING WITH GATED CONVOLUTION AND CONTEXTUAL RECONSTRUCTION LOSS
                            </p>
                            <p style="font-size: 10pt; text-align: left;">
                              2024, IEEE International Conference on Acoustics, Speech and Signal Processing（ICASSP）
                            </p>
                            <p>
                              we propose a panoramic image inpainting framework that consists of a Face Generator, a
                              Cube Generator, a side branch, and two discriminators. We use the Cubemap Projection (CMP)
                              format as network input.
                              The generator employs gated convolutions to distinguish valid pixels from invalid ones,
                              while a side branch is designed utilizing contextual reconstruction (CR) loss to guide the
                              generators to find the most suitable reference patch for inpainting the missing region.

                            </p>
                            <p>
                              <a href="https://vencoders.github.io/PII.html">[Project]</a>
                              <a href="https://arxiv.org/pdf/2402.02936">[PDF]</a>
                              <a href="https://github.com/Vencoders/Panoramic-image-inpainting">[Code]</a>
                            </p>
                          </div>
                        </td>
                      </tr>
                      

                      <tr>
                        <td style="width: 30%">
                          <div class="project-logo">
                            <a href="https://vencoders.github.io/tvqe.html"><img src="images/tvqe.jpg"
                                style="width: 100%"></a>
                          </div>
                        </td>
                        <td style="width: 70%;">
                          <div class="">
                            <p style="font-size: 18pt; text-align: left;">
                              End-to-end Transformer for Compressed Video Quality Enhancement
                            </p>
                            <p style="font-size: 10pt; text-align: left;">
                              2024, IEEE Transactions on Broadcasting
                            </p>
                            <p>
                              We propose a Transformer-based compressed video quality enhancement(TVQE) method,
                              consisting of Transformer based Spatio-Temporal feature Fusion (TSTF) module and
                              Multi-scale Channel-wise Attention based Quality Enhancement (MCQE) module.
                            </p>
                            <p>
                              <a href="https://vencoders.github.io/tvqe.html">[Project]</a>
                              <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10332936">[PDF]</a>
                              <a href="https://github.com/Vencoders/TVQE">[Code]</a>
                            </p>
                          </div>
                        </td>
                      </tr>
                    </table>

                    <h3>2023</h3>
                    <table>
                      <tr>
                        <td style="width: 30%">
                          <div class="project-logo">
                            <a href="https://vencoders.github.io/mamiqa.html"><img src="images/mamiqa1.png"
                                style="width: 100%"></a>
                          </div>
                        </td>
                        <td style="width: 70%;">
                          <div class="">
                            <p style="font-size: 18pt; text-align: left;">
                              MAMIQA: No-Reference Image Quality Assessment based on Multiscale Attention Mechanism
                              with
                              Natural Scene Statistics
                            </p>
                            <p style="font-size: 10pt; text-align: left;">
                              2023, IEEE Signal Processing Letters(SPL)
                            </p>
                            <p>
                              No-Reference Image Quality Assessment aims to evaluate the perceptual quality of an image,
                              according to human perception. This paper propose a lightweight attention mechanism using
                              decomposed large-kernel convolutions to extract multiscale features, and a novel feature
                              enhancement module to simulate HVS. We also propose to compensate the information loss
                              caused by image resizing, with supplementary features from natural scene statistics.
                              Experimental results on five standard datasets show that the proposed method surpasses the
                              SOTA, while significantly reducing the computational costs.
                            </p>
                            <p>
                              <a href="https://vencoders.github.io/mamiqa">[Project]</a>
                              <a href="https://ieeexplore.ieee.org/document/10124974">[PDF]</a>
                              <a href="https://github.com/Vencoders/MAMIQA">[Code]</a>
                            </p>
                          </div>
                        </td>
                      </tr>
                    </table>

                    <h3>2022</h3>
                    <table>
                      <tr>
                        <td style="width: 30%">
                          <div class="project-logo">
                            <a href="https://vencoders.github.io/hfcgcnn.html"><img src="images/frame4.png"
                                style="width: 100%"></a>
                          </div>
                        </td>
                        <td style="width: 70%;">
                          <div class="">
                            <p style="font-size: 18pt; text-align: left;">
                              High-frequency guided CNN for video compression artifacts reduction
                            </p>
                            <p style="font-size: 10pt; text-align: left;">
                              2022, IEEE International Conference on Visual Communications and Image Processing (VCIP)
                            </p>
                            <p>
                              This paper proposes the HFCG-CNN for video compression artifacts reduction, consisting of
                              high-frequency guidance module and quality enhancement module. The high-frequency guidance
                              module explicitly extracts the high-frequency information in the Y component. Then, the
                              high-frequency information is used in the quality enhancement module to guide the recovery
                              of all Y, U and V components. The experiment results demonstrate the effectiveness of the
                              proposed HFCGCNN method.
                            </p>
                            <p>
                              <a href="https://vencoders.github.io/hfcgcnn.html">[Project]</a>
                              <a href="https://ieeexplore.ieee.org/abstract/document/10008814/">[PDF]</a>
                              <a href="https://github.com/Vencoders/HFCG-CNN">[Code]</a>
                            </p>
                          </div>
                        </td>
                      </tr>


                      <tr>
                        <td style="width: 30%">
                          <div class="project-logo">
                            <a href="https://vencoders.github.io/natural-transfer.html"><img src="images/frame3.png"
                                style="width: 100%"></a>
                          </div>
                        </td>
                        <td style="width: 70%;">
                          <div class="">
                            <p style="font-size: 18pt; text-align: left;">
                              Neural texture transfer assisted video coding with adaptive up-sampling
                            </p>
                            <p style="font-size: 10pt; text-align: left;">
                              2022, Signal Processing: Image Communication
                            </p>
                            <p>
                              A neural texture transfer-assisted video coding with an adaptive up-sampling scheme is
                              proposed in this paper. This scheme adaptively decides whether a frame should be
                              down-sampled or not. In the decoder, the down-sampled frames are restored by exploring
                              their correlations with the frames that are not down-sampled using neural texture transfer
                              in a multi-scale manner.
                            </p>
                            <p>
                              <a href="https://vencoders.github.io/natural-transfer.html">[Project]</a>
                              <a href="https://weizequan.github.io/SPIC2022/paper.pdf">[PDF]</a>
                              <a href="https://github.com/Vencoders/Ref-frame-encode">[Code]</a>
                            </p>
                          </div>
                        </td>
                      </tr>
                    </table>
                </div>

              </div>
            </div>
          </div>


        </div>
      </div>
    </div>
  </div>
</div>


</div>
</body>
</html>
